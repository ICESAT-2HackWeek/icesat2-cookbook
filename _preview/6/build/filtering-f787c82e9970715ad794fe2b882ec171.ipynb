{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78cee78b",
   "metadata": {},
   "source": [
    "# Cloud effects on ICESat-2 data and Data Filtering \n",
    "\n",
    "## Author(s)\n",
    "\n",
    "Ben Smith\n",
    "\n",
    "# 1. Clouds over land ice: What problems they cause, and what to do about it.\n",
    "This tutorial covers some of the reasons you might see weird results over ice when clouds start to blot out the surface signals.  The learning objectives I'd like to get to are:\n",
    "\n",
    "* Understanding how clouds affect laser-altimetry signals\n",
    "\n",
    "* Recognizing how these effects are manifest in the ATL06 product\n",
    "\n",
    "* Gaining familiarity with the ATL06 parameters that can identify cloudy returns\n",
    "\n",
    "This part of the tutorial will focus on clouds and data-quality problems that cause gross errors in surface-height estimates.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fad61aa2-26aa-4683-94a3-b382ab48d5f9",
   "metadata": {},
   "source": [
    "### 1.1  Background: ATL06 signal processing\n",
    "\n",
    "<img src=\"assets/filtering/ATL06_segment_model.png\"  width=600 height=600 alt=\"ATL06 segment model\"/>\n",
    "\n",
    "Recall that ATL06 gives us surface heights based on the heights of collections of photons in a 40-m (horizontal) by w_surface_window (vertical) window.  It uses a variety of techniques to shrink the window to the smallest size that contains the surface without clipping off any signal photons.  \n",
    "\n",
    "\n",
    "There's a general philosophy that went into the design of ATL06:\n",
    "1. Use the best available information to identiy the surface\n",
    "2. If there's a chance that we've found a weak surface return, report it\n",
    "3. Provide enough parameters that users can decide which returns are worth using, and which are not\n",
    "\n",
    "When there are no thick clouds between ICESat-2 and the surface, finding the surface return and reporting its height is straightforward: ATL03 provides a tight cluster of high-confidence photons, and ATL06 calculates a weighted average of their heights.\n",
    "\n",
    "Once clouds start to block some of the laser light, the number of photons that return to ATLAS from each return becomes progressively smaller.  Unfortunately, even if there are no laser photons to measure, during daylight there is no shortage of other photons to track, which come primarily from sunlight reflecting ground and from clouds.  ATLAS does a very good job of filtering out almost all of these photons, but on a sunny day, over a white surface, the measured background rate can be as high as 12 MHz.  Converting to dimensions that we'll be seeing this is:\n",
    "$$ \n",
    "    \\frac{1.2\\times10^7 photons}{second} \\times \\frac{1 second}{1.5\\times10^8 m} = \\frac{1 photon}{12.5 m}\n",
    "$$\n",
    "This doesn't sound like a lot, but over a 10-meter-high window that's 40 m long (typical for the kind of windows you might use to look for the surface if you didn't know where to find it) we can expect to find 45 photons.\n",
    "\n",
    "Unlike surface-return photons, background photons are uniformally distributed in height, and any clustering of these photons will be due to random chance.  When the signal quality is marginal, ATL03 may flag photons only as low- and medium- confidence for a particular segment, or may flag no photons at all.  If ATL03 hasn't told ATL06 which photons are the surface, the algorithm uses a backup signal-finding strategy that initializes surface finding using the strongest cluster of photons available.  It then attempts to converge its surface window on a tight cluster of photons.  This occasionally works, but if there really is no signal, the size of the window generally remains large, and we can evaluate the results based on the signal-to-noise ratio (SNR) of whatever ends up inside the window.  Only those segments with at least 10 photons, for which the probability of converging to an SNR equal to the observed SNR or better for random-noise inputs is less thatn 5% are reported.  This cuts down on false positives considerably. \n",
    "\n",
    "Let's take a look at some data from Antarctica to see what different problematic data might look like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2cfedf-31d8-483e-bdae-a25ffb0b516c",
   "metadata": {},
   "source": [
    "# 1.1 Setup\n",
    "\n",
    "We'll be working on and around Cook Ice Shelf, in East Antarctica.  We'll use the SlideRule computational platfrom to access ATL06 segments and ATL03 photons, and will use an image of the ice shelf from the REMA project to visualize the location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2b5df-65e6-408a-8629-1d6154e68283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages for the tutorial\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "%matplotlib widget\n",
    "\n",
    "# packages needed for the basemap \n",
    "import PIL.Image\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "#sliderule\n",
    "from sliderule import icesat2\n",
    "import sliderule\n",
    "icesat2.init(\"slideruleearth.io\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d3747-1daf-4154-a4df-539bd322cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a polygon defining our area of interest:\n",
    "df = gpd.read_file('assets/filtering/Cook_subset.kml')\n",
    "geom_xy = df.to_crs(3031).geometry[0]\n",
    "geom_ll = df.geometry[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceddc07-84c8-4011-a4cd-1b0976f51401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a shaded relief DEM to use as a basemap. This uses the ArcticDEM image server.\n",
    "\n",
    "HOST = 'https://elevation2.arcgis.com/arcgis/rest/services/Polar/AntarcticDEM/ImageServer/exportImage?'\n",
    "params = []\n",
    "params.append('bboxSR=3031')\n",
    "params.append('imageSR=3031')\n",
    "params.append('format=jpgpng')\n",
    "params.append('noDataInterpretation=esriNoDataMatchAll')\n",
    "params.append('f=image')\n",
    "\n",
    "[xmin, xmax], [ymin, ymax] = np.array(geom_xy.bounds).reshape(2,2).T\n",
    "# include a little bit more area at the top:\n",
    "ymax += 2.e4 \n",
    "bbox = f'{xmin},{ymin},{xmax},{ymax}'\n",
    "params.append(f'bbox={urllib.parse.quote(bbox)}')\n",
    "\n",
    "image_service_url = HOST + '&'.join(params)\n",
    "shaded_relief_img = {'data': np.array(PIL.Image.open(urllib.request.urlopen(image_service_url))),\n",
    "    'extent':[xmin, xmax, ymin, ymax]}\n",
    "plt.figure()\n",
    "plt.imshow(shaded_relief_img['data'], extent=shaded_relief_img['extent'])\n",
    "x_poly, y_poly, _ = [*np.c_[geom_xy.exterior.coords].T]\n",
    "plt.plot(x_poly, y_poly,'r')\n",
    "\n",
    "plt.gca().set_xlabel('polar-stereographic E, m')\n",
    "plt.gca().set_xlabel('polar-stereographic N, m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f262e89-bb48-42df-bf20-f7325b5b2e71",
   "metadata": {},
   "source": [
    "\n",
    "# 1.2 Geographic Setting\n",
    "\n",
    "We'll be working on Cook Ice Shelf, a heavily crevassed and rifted ice shelf that drains East Antarctica.  I've defined a polygon that covers the transition between grounded ice (at the top) floating ice (within the polygon).  The region we'll be working in is about 70 km x 60 km, and is covered by 20-30 ICESat-2 repeat tracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0549195-2533-4b65-8c0b-e618962a9dcb",
   "metadata": {},
   "source": [
    "# 2. ICESat-2 data over Cook Ice Shelf\n",
    "\n",
    "For this tutorial, we'll use the SlideRule service to download ATL06 (height measurements) and ATL03 (photon clouds).  SlideRule offers a quick and efficient way to search for and access data for limited areas of the ice sheet without the need to subset or download entire granules of data.  \n",
    "\n",
    "\n",
    "## 2.1 Preliminary data download\n",
    "\n",
    "Let's begin by looking at the distibution of tracks for the region, using the ATL06 (land-ice height) product. We'll first request a subset of data covering 2020.  To run SlideRule, we need a polygon (defined as a list of dictionaries, each of which has a 'lat' and a 'lon' element) and a start and stop time.  We'll use the ATL06sp endpoint, which subsets all the granules in the region for a standard set of ATL06 fields.  The polygon we're using comes from a kml file called Cook_subset.kml, which is in the assets/filtering directory.  You can substitute your own file if you want to work on a different area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080cbbc-382b-4bde-8b9b-0ced479372ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a polygon in sliderule format.\n",
    "poly=[{'lat':jj[1], 'lon':jj[0]} for jj in geom_ll.exterior.coords]\n",
    "\n",
    "#Define the request (a dictionary of parameters, \n",
    "#see: https://slideruleearth.io/web/rtd/user_guide/icesat2.html#parameters\n",
    "sliderule_parms= {\n",
    "    \"poly\":poly,\n",
    "    \"t0\": \"2020-01-01T01:00:00Z\",\n",
    "    \"t1\": \"2021-01-01T01:01:01Z\",\n",
    "  }\n",
    "# submit the request, which should take 10-15 seconds\n",
    "D6=icesat2.atl06sp(sliderule_parms)\n",
    "# transform the output to polar stereographic coordinates\n",
    "D6.to_crs(3031, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24daef41-929b-4db3-93cd-573e6733eb57",
   "metadata": {},
   "source": [
    "We can take a look at the data that SlideRule returned to see what fields are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70deb08-cea2-4fe9-8f03-037f4ab4b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1c143-55a8-4f46-b1c0-8a379f9c4415",
   "metadata": {},
   "source": [
    "There are 21 columns here, which describe:\n",
    "\n",
    "* segment location (geometry)\n",
    "\n",
    "* segment location along-track coordinates ( x_atc, y_atc )\n",
    "\n",
    "* surface height and accuracy (h_li, h_li_sigma, atl06_quality_summary) \n",
    "\n",
    "* surface slope (dh_fit_dx, dh_fit_dy)\n",
    "\n",
    "* signal (r_eff, n_fit_photons)\n",
    "\n",
    "* track and timing parameters (cycle, gt, cycle, spot, delta_time)\n",
    "\n",
    "The list of fields is designed to cover almost all users' needs, and we'll only end up using a few of them in this tutorial.  Let's look at the elevations (h_li) that came back in the current request in map view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c3447-e19f-447a-9562-9029a887f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax_scat = plt.gca()\n",
    "plt.imshow(shaded_relief_img['data'], extent=shaded_relief_img['extent'])\n",
    "\n",
    "xx=np.array(D6.geometry.x)\n",
    "yy=np.array(D6.geometry.y)\n",
    "hh = np.array(D6.h_li)\n",
    "\n",
    "# sort the data by abs(height) to make the outliers show up\n",
    "ii=np.argsort(np.abs(hh))\n",
    "ii=ii[np.isfinite(hh[ii])]\n",
    "h_scat=plt.scatter(xx[ii], yy[ii], 3,  c=hh[ii], clim=[-200, 200], cmap='RdBu')\n",
    "plt.colorbar(h_scat, label='h_li')\n",
    "\n",
    "# suggested area to look at a poorly performing segment:\n",
    "xl, yl = [np.array([1079194.72014081, 1080624.2833137 ]), np.array([-2089358.21884508, -2088030.7673274 ])]\n",
    "\n",
    "plt.plot(xl[[0, 0, 1, 1, 0]], yl[[0, 1, 1, 0, 0]],'--', color='palegreen', marker='*', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b493e1-e00a-417b-a88e-8b5b27d4512d",
   "metadata": {},
   "source": [
    "We can see on this plot that most of the surface heights are close to sea level, but a few tracks are reporting heights that are probably far too high.\n",
    "\n",
    "## 2.1 Selecting a track that has cloud problems\n",
    "\n",
    "If you see a plot like this where there is a track that has problems, you need to figure out which track is affected.  To do this, you need to know which RGT (repeat ground track) has the problem, and which beams from that track are affected.\n",
    "\n",
    "One way to do this is to use the zoom tool to find a track with large positive h_li values, and capture the axes x_lim and y_lim properties, then use the dataframe's _cx_ method to capture the subset of the dataframe within the axes.  You can do this by setting _'interactive'_ to _True_ in the next cell.\n",
    "\n",
    "For puruposes of making the tutorial run without any interaction, I've also defined some limits in the next cell, and I'll be talking about the results that come from those limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063da18-feab-48d8-ac9a-164ce10e5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if interactive is False, we'll use the xl, yl defined in the previous code cell\n",
    "interactive=False\n",
    "\n",
    "if interactive:\n",
    "    xl=ax_scat.get_xlim() ; yl=ax_scat.get_ylim()\n",
    "D6_sub= D6.cx[xl[0]:xl[1], yl[0]:yl[1]]\n",
    "D6_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14974928-8467-4a5c-aeaa-5dd038961eb1",
   "metadata": {},
   "source": [
    "If you run this for the default region, you'll see that there is one rgt (1259), and two gts (10 and 20) in the polygon.  Let's do a plot of the height against the along-track coordinate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08f527-d09f-49a6-841a-4baa94ba79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "hs=plt.scatter(D6_sub.x_atc, D6_sub.h_li,5, c=D6_sub.cycle)\n",
    "plt.colorbar(hs, label='cycle_number')\n",
    "plt.gca().set_xlabel('along-track x (x_atc), m')\n",
    "plt.gca().set_ylabel('h, m')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41819a-b751-40e9-bcd0-f49667c15d54",
   "metadata": {},
   "source": [
    "This plot shows elevations near zero for cycle 8, and elevations far above and below the surface for cycles 7 and 9.  If we zoom in on cycle 8, we can see something that looks like ice-shelf terrain, so we can guess that this is probably \"right\" and cycles 7 and 9 are wrong.  \n",
    "\n",
    "## 2.2  Getting ATL03 data for a problematic track\n",
    "\n",
    "The next step is to look at the ATL03 photon cloud for each cycle to see what has worked and what has not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc7da45-dc51-4dc4-ba8e-d539912bcf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select your track here! \n",
    "this_rgt=1259\n",
    "this_track=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a3533-d86d-4b73-9bbb-cd058d255392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ATL03.  this should take <1 minute to run.\n",
    "\n",
    "icesat2.init(\"slideruleearth.io\", True)\n",
    "sliderule_ATL03_parms= {\n",
    "    \"poly\":poly, \n",
    "    \"t0\": \"2020-01-01T01:00:00Z\",\n",
    "    \"t1\": \"2021-01-01T01:00:00Z\",\n",
    "    \"rgt\":this_rgt,\n",
    "    \"srt\":icesat2.SRT_LAND_ICE,\n",
    "    \"cnf\":-2,\n",
    "    \"track\": this_track,\n",
    "  }\n",
    "# run sliderule for track\n",
    "D3=sliderule.run('atl03x', sliderule_ATL03_parms).to_crs(3031)\n",
    "\n",
    "# make a subset for our geographic region\n",
    "D3_sub = D3.cx[xl[0]:xl[1], yl[0]:yl[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa9f7f-7a2f-4897-a228-5cfc3287650f",
   "metadata": {},
   "source": [
    "Let's plot the photons from ATL03, and the segments from ATL06 for each beam in pair 1 for each cycle.  \n",
    "\n",
    "Note that SlideRule has numeric codes for the different groundtracks, so \n",
    "\n",
    "* gt=10 -> gt1l\n",
    "  \n",
    "* gt=20 -> gt1r\n",
    "\n",
    "The SlideRule icesat2 module has 'GT1L','GT1R','GT2L' attributes that provide these codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00230ed4-4755-442d-88d6-4a081e3d4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf, hax= plt.subplots(3, 2, layout='constrained', sharex=True, figsize=[8, 8])\n",
    "\n",
    "for ax1, cycle in zip(hax, np.unique(np.array(D6_sub.cycle))):\n",
    "    for ax, gt in zip(ax1, np.unique(np.array(D6_sub['gt']))):\n",
    "        i3=(D3_sub['gt']==gt) & (D3_sub.cycle==cycle)\n",
    "        ax.plot(D3_sub.x_atc[i3], D3_sub.height[i3],'k.', markersize=1, label='ATL03')\n",
    "        i6=(D6_sub['gt']==gt) & (D6_sub.cycle==cycle)\n",
    "        ax.plot(D6_sub.x_atc[i6], D6_sub.h_li[i6],'.', color='steelblue', label='ATL06')\n",
    "        ax.set_title(f'cycle={cycle}, gt={gt}')\n",
    "hax[1, 1].legend()\n",
    "\n",
    "for ax in hax[2,:]:\n",
    "    ax.set_xlabel('x_atc, m')\n",
    "for ax in hax[:,0]:\n",
    "    ax.set_ylabel('WGS84 height, m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7b8a7-6a5f-4642-a8d6-3d1687efe68b",
   "metadata": {},
   "source": [
    "We can see a few things going on here.  The photons in ATL03 come from vertical bands of data (_telemetry bands_) that the algorithms running onboard ATLAS have identified as possibly containing a ground return.  In some cases the algorithm is correct (e.g. cycle 8), in other cases it returns only a band of photons far from the ground (gt 10 for cycles 7 and 9) or returns a band of photons that contains the ground for which there is no usable return (gt 20 for cycles 7 and 9).\n",
    "\n",
    "We can also see bands of photons above the ground in gt 10 for cycles 7 and 8: these are most likely clouds, and the photons in these bands are much farther apart than the photons from true ground returns (see cycle 8).\n",
    "\n",
    "All of the tracks contain at least some background photons, which are uniformly distributed over the telemetry bands.  If you check the dates for the different cycles, you'll see that cycle 7 comes from June (Antarctic winter), cycle 8 comes from mid September (late winter / early spring) and cycle 9 comes from mid December (high summer).  As a result, there are very few background photons in cycle 7, and very many in cycle 9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cdb4d0-2332-4787-8a0d-8e41c2451900",
   "metadata": {},
   "source": [
    "It's instructive to see how the ATL03 classification algorithm sees these points.  Let's do the plot again, but color code by signal classification.  These are stored in the ATL03 atl03_cnf field.  the confidence values are:\n",
    "\n",
    "* -- -1: Events not associated with a specific surface type\n",
    "* --  0: noise\n",
    "* --  1: noise photons within a +-10 m vertical buffer of a detected photon\n",
    "* --  2: low confidence signal\n",
    "* --  3: medium confidence signal\n",
    "* --  4: high confidence signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c22222-0ab2-4e9c-89ef-849699d26fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf, hax= plt.subplots(3, 2, layout='constrained', sharex=True, figsize=[8, 8])\n",
    "\n",
    "for ax1, cycle in zip(hax, np.unique(np.array(D6_sub.cycle))):\n",
    "    for ax, gt in zip(ax1, np.unique(np.array(D6_sub['gt']))):\n",
    "        for cnf, name, color in zip([-1, 0, 1, 2, 3, 4], ['unclassified', 'noise','buffer','low','medium','high'],['gray', 'black','skyblue','green','orange','red']):\n",
    "            i3=(D3_sub['gt']==gt) & (D3_sub.cycle==cycle) & (D3_sub.atl03_cnf==cnf)\n",
    "            ax.plot(D3_sub.x_atc[i3], D3_sub.height[i3],'.', markersize=1, color=color, label=name)\n",
    "        ax.set_title(f'cycle={cycle}, gt={gt}')\n",
    "hax[1, 0].legend()\n",
    "\n",
    "for ax in hax[2,:]:\n",
    "    ax.set_xlabel('x_atc, m')\n",
    "for ax in hax[:,0]:\n",
    "    ax.set_ylabel('WGS84 height, m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98abd96c-973d-4c11-b37d-6e2fdbe4fca8",
   "metadata": {},
   "source": [
    "If you zoom around a little bit, you'll see that cycles 7 and 9 have essentially all photons classified as noise, but cycle 8 has a mixture of high-confidence and (for gt==20) medium/low photons.  The low - confidence photons come from places where the surface is unusually dim, probably because of cloud attenuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566c0d1-e94d-43cf-a42c-1dc61ce20b04",
   "metadata": {},
   "source": [
    "## 2.3 Relating cloud effects to ATL06 data parameters\n",
    "\n",
    "Our problem comes when the ground return is not strong enough to trigger the signal finder, and we start to see triggers associated with:\n",
    "- Cloud tops\n",
    "- Random clusterings of background photons\n",
    "\n",
    "These should both be statistically distinct from surface returns because:\n",
    "- The returns are less intense than a high-quality surface return\n",
    "- The photons are more widely vertcally spread than those in surface returns\n",
    "- The surface window cannot converge on a small vertical window around the surface\n",
    "- Surface heights and slopes are not consistent between adjacent segments\n",
    "\n",
    "There are a few ATL06 parameters that help quantify these distinctions. \n",
    "- h_li_sigma : the estimated error in the surface-height estimate\n",
    "- n_fit_photons : The number of photons found in each segment\n",
    "- w_surface_window_final : The size of the converged surface window\n",
    "- h_robust_sprd : A percentile-based estimate of the spread of the photons, corrected for background\n",
    "- snr : the observed signal-to-noise ratio for the selected photons\n",
    "- snr_significance : The estimated probability that a random clustering of photons would produce the observed SNR\n",
    "- dh_fit_dx : the along-track segment slope\n",
    "- r_eff : the effective reflectance of the surface\n",
    "\n",
    "There's one more parameter that puts a few of these ideas together, in /gtxx/land_ice_segments:\n",
    "- atl06_quality_summary : a combination of parameters (h_li_sigma, n_fit_photons/w_surface_window_final, and signal_selection_source).  Zero indicates a good segment, 1 indicates a possibly bad segment.\n",
    "\n",
    "For most purposes selecting those points for which atl06_quality_summary==0 will filter out most of the bad returns.  Let's regenerate the map of Cook ice shelf to see how this works:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddef248-8c76-4976-b358-f59a0ba7a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax_scat_clean = plt.gca()\n",
    "plt.imshow(shaded_relief_img['data'], extent=shaded_relief_img['extent'])\n",
    "\n",
    "xx=np.array(D6.geometry.x)\n",
    "yy=np.array(D6.geometry.y)\n",
    "hh = np.array(D6.h_li)\n",
    "qq = np.array(D6.atl06_quality_summary)\n",
    "\n",
    "# sort the data by abs(height) to make the outliers show up\n",
    "ii=np.argsort(np.abs(hh))\n",
    "# here is where we filter out the atl06_quality_summary==1 points\n",
    "ii=ii[np.isfinite(hh[ii]) & (qq[ii]==0)]\n",
    "h_scat_clean=plt.scatter(xx[ii], yy[ii], 3,  c=hh[ii], clim=[-200, 200], cmap='RdBu')\n",
    "plt.colorbar(h_scat_clean, label='h_li', extend='both')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c961d-abdd-4b70-bafc-08ff812fd96a",
   "metadata": {},
   "source": [
    "Presto!  all of the too-high and too-low returns from the first map are gone!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2297664e-8e3d-40ec-943f-59a5bd17c8b1",
   "metadata": {},
   "source": [
    "# 3. Data exploration\n",
    "\n",
    "\n",
    "If you have time, take a few minutes to go back to the first cell after 4.1 and set 'interactive' to True.  Then you can try zooming in on the map to identify the different tracks, and downloading the ATL03 to go with them (see the cell marked \"select your track here\").  You should be able to find:\n",
    "\n",
    "- Clouds\n",
    "- Crevasses\n",
    "- Rifts in the ice sheet\n",
    "- Sea ice or open water.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b6559-7d1a-41c8-9415-3c09fc8262ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icesat2-cookbook-dev",
   "language": "python",
   "name": "icesat2-cookbook-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
